---
title: "FIMS software platform exploratory analysis"
author: "Andrea Havron"
date: "7/20/2021"
output: html_vignette
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(magrittr)
library(tidyr)
library(ggplot2)
options(knitr.table.format = "latex") 
```


# Introduction

The Fisheries Integrated Management Systems aims to offer a system of modular, maintainable, and extensible models for next generation stock assessments. The choice of software platform used to develop FIMS will heavily influence the success of the initiative. While a thorough review of FIMS requirements is currently underway, a concurrent exploration of candidate software platforms will provide insight that will guide final decisions once software requirements have been established.

This project aims to produce a comprehensive review of available statistical computing platforms and summarize their suitability with respect to FIMS. The cross-comparison study will focus on state-space and explicit spatio-temporal modeling. Models will be developed and implemented across the following software platforms: TMB, C++, Julia, Greta tensorflow, and Stan. Inference pathways for both frequentist and Bayesian inference will be outlined for each platform when applicable. Under consideration will be performance, speed, the usability of the software with respect to both the user and developer, and software features that meet FIMS requirements under consideration.

# Models

The state-space comparison looks at two models: a simple Gompertz state-space model and a logistic growth model.

Gompertz:

$$\begin{align}
\eta_{t} &= \theta_{1} + \theta_{2}u_{t-1} \\
u_{t} &\sim N(\eta_{t},\sigma^{2}_{proc}) \\
y_{t} &\sim N(u_{t}, \sigma^{2}_{obs})
\end{align}$$,

where $\sigma^{2}_{proc}$ represents the process variance and $\sigma^{2}_{obs}$ represents the observation variance.

Logistic growth:

$$\begin{align}
\eta_{t} &= u_{t-1} + \theta_{1}u_{t-1}(1-u_{t-1}/\theta_{2}) \\
u_{t} &\sim LN(log(\eta_{t}),\sigma^{2}_{proc}) \\
y_{t} &\sim LN(log(u_{t}), \sigma^{2}_{obs})
\end{align}$$,

where $\theta_{1}$ represents the growth rate and $\theta_{2}$ represents the density-dependent carrying capacity.

# Software Platforms
## TMB/R
Fast random effects, reliance on small community of developers

* Base code written in C++, models implemented in R
* CppAD used for forward/reverse autodifferentiation
* Derivatives out to third order calculated for Laplace approximation
* LA used to integrate out random effects
* Sparse matrix algorithms used to automatically detect sparsity in Hessian and calculate sparse cholesky decompostion
* Can fit models with large number of parameters and random effects
* Memory bottleneck when translating from R to C++ (MakeADFun) and returning parameter/derived standard errors (sdreport)
* ?Modularity
* Do not need to be expert C++ programmer to write TMB models, although contributors need more advanced skills to write efficient code or contribute to TMB base
* Well tested AD capabilities
* Small developers community

Julia
Fast, Code written and executed in single language, still in infancy (may be bugs in base code, small community of developers)

* New dynamic programming language with Just-In-Time (JIT) Compiler
* Fast although first time to plot can be slow
* Avoids the ‘two-language problem’ – all code is written and executed with Julia
* Modular language – development occurs within packages
* Multiple packages useful to numerical optimization: Distributions.jl, SPDEPrecisionMatrices.jl, GaussianMarkovRandomFields.jl, Optim.jl, SparsityDetection.jl
* Autodifferentiation packages: ForwardDiff.jl, ReverseDiff.jl, Zygote.jl
* Bayesian package: Turing.jl
* Higher order differentiation to use Laplace approximation to integrate out parameters is still under development

C++/Rcpp
Examples: r4MAS, CASAL, GADGET, MultifanCL

Stan
HMC sampler, Laplace approximation using the adjoint method, variational Bayes

* TMBstan
* Writing code in Stan versus interacting with C++

Greta
Machine learning approach to Bayesian inference

* Relies on tensorflow to computationally automate Baye’s Rule in its application to generative DAGs
* greta is a CRAN package that interfaces between R and tensorflow
* tensorflow is a python package that implements machine learning

## Project Deliverables
      
* Table detailing whether or not each platform meets list of FIMS requirements
* Speed test results
* Outline inference pathways

## Results

### Platform Comparisons

#### State-Space Gompertz Results
```{r tableSSgomp, echo = FALSE, warnings = FALSE}
load('../results/gompertz.RData')
df <- as.data.frame(cbind(true=c(2,0.8,0.1,0.5),sapply(gompertz.results, function(x) x$par.est)))
row.names(df) <- c('$\\theta_{1}$', '$\\theta_{2}$', '$\\sigma_{proc}$','$\\sigma_{obs}$')
kbl(df,booktabs = TRUE, caption = 'Parameter estimates for state-space gompertz model (n=100)', digits = 3, label = 'SSgompMu')

df <- as.data.frame(sapply(gompertz.results, function(x) x$se.est))
row.names(df) <- c('$\\theta_{1}$', '$\\theta_{2}$', '$\\sigma_{proc}$','$\\sigma_{obs}$')
kbl(df,booktabs = TRUE, caption = 'Standard error estimates for state-space gompertz model (n=100)', digits = 3, label = 'SSgompSE')

df <- data.frame('time (min)' = sapply(gompertz.results, function(x) x$time))
kbl(df,booktabs = TRUE, caption = 'Run times for state-space gompertz model (n=100)', digits = 3, label = 'SSgompTime')
```


#### State-Space logistic growth results
```{r tableSSlogisticMean, echo = FALSE, warnings = FALSE}
load('../results/logistic.RData')
df <- as.data.frame(cbind(true=c(0.2,100L,0.01,0.001),sapply(logistic.results, function(x) x$par.est)))
row.names(df) <- c('r', 'K', '$\\sigma_{proc}$','$\\sigma_{obs}$')
kbl(df,booktabs = TRUE, caption = 'Parameter estimates for state-space logistic model (n=100)', label = 'SSlogMu', format.args = list(scientific = FALSE))

df <- as.data.frame(sapply(logistic.results, function(x) x$se.est))
row.names(df) <- c('r', 'K', '$\\sigma_{proc}$','$\\sigma_{obs}$')
kbl(df,booktabs = TRUE, caption = 'Standard error estimates for state-space logistic model (n=100)', digits = 3, label = 'SSlogSE')

df <- data.frame('time (min)' = sapply(logistic.results, function(x) x$time))
kbl(df,booktabs = TRUE, caption = 'Run times for state-space logistic model (n=100)', digits = 3, label = 'SSlogTime')
```

#### Benchmark results for logistic growth model
```{r, echo = FALSE, warnings = FALSE, fig.height = 6, fig.width = 6}
n.seq <- seq(5,11,1)
plot.res <- c()
for(i in 1:length(n.seq)){
  n <- 2^n.seq[i]
  load( paste0('../results/logistic/logistic', '_n', n, '.RData'))
  plot.res <- rbind(plot.res, sapply(logistic.results, function(x) x$meanESS)[2:4]/
    sapply(logistic.results, function(x) x$time)[2:4])
  plot.res <- rbind(plot.res, sapply(logistic.results, function(x) x$minESS)[2:4])
  plot.res <- rbind(plot.res, sapply(logistic.results, function(x) x$time)[2:4])
}
colnames(plot.res) <- names(logistic.results)[2:4]
plot.res %<>% as.data.frame()
plot.res$metric <- rep(c('MCMC efficiency', 'min ESS', 'time'), length(n.seq))
plot.res$nsamp <- rep(2^n.seq, each = 3)
plot.res %>% 
  pivot_longer(., 1:3, names_to = 'model', values_to = 'value') %>%
  ggplot(., aes(x=nsamp, y=value,col=model)) + geom_line() + 
  theme_classic() + facet_wrap(~metric, scales = 'free', ncol=1)

```



### Inference Pathways

TMB

1. Frequentist: -> fn() and gr() passed to nlminb in R -> quasi-Newton optimization routine (Fox, Hall, and Schryer, 1978; Fox, 1997)
    + fn(): inner optimization step finds optimum random effects values; Laplace approximation calculates the marginal likelihood after integrating out random effects
    + gr(): Gradient functions are determined using forward/reverse automatic differentiation from cppAD and tinyAD
2. Bayesian -> fn() and gr() passed to Stan -> HMC algorithm (using tmbStan)

C++

1. Frequentist
2. Bayesian -> fn() and gr() passed to Stan -> HMC algorithm (using tmbStan)
    + -> fn() and gr() passed to Stan -> HMC algorithm (using tmbStan)
    + ? -> fn() passed to Stan -> Laplace/HMC ?
    + ? -> pass to python and tensorflow ?
    + Framework for acquiring gradients?

Performance results (eg. stability of source code)
  *       Software platform pros and cons
  *       Ease of use summary
  *       Summary of input/output  procedures

